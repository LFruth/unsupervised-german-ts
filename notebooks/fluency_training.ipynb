{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook for the LM-Fluency model\n",
    "\n",
    "The code is from *The Summary Loop* https://github.com/CannyLab/summary_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wiki_paragraphs = pd.read_csv(\"../data/all_wiki_paragraphs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>687486</th>\n",
       "      <td>Johann Theodor von Scheffer</td>\n",
       "      <td>Großen politischen Einfluss gewann Scheffer un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786382</th>\n",
       "      <td>Krankenhaus West (Stralsund)</td>\n",
       "      <td>Die Klinikumskirche, das Gebäude 20 der Anlage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77804</th>\n",
       "      <td>Washington Capitals</td>\n",
       "      <td>Im NHL Entry Draft 2004 durften die Capitals a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046720</th>\n",
       "      <td>Gerrit Willemsz Horst</td>\n",
       "      <td>Im \"Lexikon der holländischen Stillebenmaler\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131753</th>\n",
       "      <td>Kloster Mariastein</td>\n",
       "      <td>Eine Legende berichtet, dass ein kleiner Hirte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title  \\\n",
       "687486    Johann Theodor von Scheffer   \n",
       "786382   Krankenhaus West (Stralsund)   \n",
       "77804             Washington Capitals   \n",
       "1046720         Gerrit Willemsz Horst   \n",
       "131753             Kloster Mariastein   \n",
       "\n",
       "                                                 paragraph  \n",
       "687486   Großen politischen Einfluss gewann Scheffer un...  \n",
       "786382   Die Klinikumskirche, das Gebäude 20 der Anlage...  \n",
       "77804    Im NHL Entry Draft 2004 durften die Capitals a...  \n",
       "1046720  Im \"Lexikon der holländischen Stillebenmaler\" ...  \n",
       "131753   Eine Legende berichtet, dass ein kleiner Hirte...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_paragraphs = wiki_paragraphs.sample(frac = 1)\n",
    "wiki_paragraphs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = wiki_paragraphs['paragraph'][:int(len(wiki_paragraphs)*0.9)].to_numpy()\n",
    "eval_d = wiki_paragraphs['paragraph'][int(len(wiki_paragraphs)*0.9):].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers.optimization import AdamW\n",
    "\n",
    "# Loading Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"dbmdz/bert-base-german-cased\").to('cuda')\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb, os\n",
    "\n",
    "wandb.init(project=\"Fluency Finetune\")\n",
    "wandb.run.name = \"test\"\n",
    "wandb.run.save()\n",
    "\n",
    "split_size = len(train_d) / 8\n",
    "\n",
    "eval_loss_min = 10000000\n",
    "\n",
    "for n_epoch in range(5):\n",
    "    b_idx = 0\n",
    "    for batch in np.array_split(train_d, split_size):\n",
    "        b_idx += 1\n",
    "        print(\"Training Batch #\", b_idx)\n",
    "\n",
    "        # tokenize\n",
    "        inputs = tokenizer(batch.tolist(), return_tensors=\"pt\", truncation=True, padding=\"longest\")\n",
    "        inputs['labels'] = inputs[\"input_ids\"].detach().clone()\n",
    "\n",
    "        # randomly choosing words to mask\n",
    "        rand = torch.rand(inputs.input_ids.shape)\n",
    "        mask_arr = (rand < 0.15) * (inputs.input_ids != 0) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 103)\n",
    "\n",
    "        # mask words\n",
    "        for i in range(len(inputs.input_ids)):\n",
    "            for j in range(len(inputs.input_ids[i])):\n",
    "                if mask_arr[i][j]:\n",
    "                    inputs.input_ids[i][j] = 104\n",
    "\n",
    "        inputs.to(\"cuda\")\n",
    "\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if b_idx%5 == 0: # optimize every 5 steps\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        wandb.log({'train_loss': loss})\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if b_idx%20 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                e_idx = 0\n",
    "                losses = 0\n",
    "                for i in range(5):\n",
    "                    e_idx += 1\n",
    "                    \n",
    "                    eval_texts = eval_d[0:10].tolist()\n",
    "                    \n",
    "                    with torch.autocast(\"cuda\"):\n",
    "                        # tokenize\n",
    "                        inputs = tokenizer(eval_texts, return_tensors=\"pt\", truncation=True, padding=\"longest\")\n",
    "                        inputs['labels'] = inputs[\"input_ids\"].detach().clone()\n",
    "\n",
    "                        rand = torch.rand(inputs.input_ids.shape)\n",
    "                        mask_arr = (rand < 0.15) * (inputs.input_ids != 0) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 103)\n",
    "\n",
    "                        # mask words\n",
    "                        for i in range(len(inputs.input_ids)):\n",
    "                            for j in range(len(inputs.input_ids[i])):\n",
    "                                if mask_arr[i][j]:\n",
    "                                    inputs.input_ids[i][j] = 104\n",
    "                        inputs.to(\"cuda\")\n",
    "                        with torch.autocast(\"cuda\"):\n",
    "                            outputs = model(**inputs)\n",
    "                            loss = outputs.loss\n",
    "                    losses += loss\n",
    "\n",
    "                losses /= e_idx\n",
    "                print(\"Eval Loss: %.3f\" % (losses))\n",
    "\n",
    "            if losses < eval_loss_min:\n",
    "                eval_loss_min = losses\n",
    "                model_output_file = os.path.join(\"E:/models/\", \"distilbert_wiki_finetune.bin\")\n",
    "                torch.save(model.state_dict(), model_output_file)\n",
    "\n",
    "            wandb.log({'eval_loss': loss})\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
